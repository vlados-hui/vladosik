#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import os
import json
import time
import random
from fake_useragent import UserAgent
import cloudscraper
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('parser.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

class FinnNoParser:
    def __init__(self):
        self.base_url = "https://www.finn.no"
        self.ua = UserAgent()
        self.headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5,nb;q=0.3',
            'Referer': 'https://www.google.com/',
            'Accept-Encoding': 'gzip, deflate, br'
        }
        self.session = self.create_session()
        self.scraper = self.create_scraper()
        self.max_ads = 150
        self.ads_count = 0
        self.filters = self.setup_filters()
        self.proxies = self.load_proxies()
        self.blacklist = self.load_blacklist()
        self.proxy_rotation_counter = 0
        self.proxy_rotation_threshold = random.randint(3, 8)
        self.start_time = time.time()
        self.success_count = 0
        self.error_count = 0

    def setup_filters(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å"""
        filters = {}
        
        print("\n" + "="*50)
        print("üí∞ –í–≤–µ–¥–∏—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω—ã —Ç–æ–≤–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 100-80000):")
        price_input = input("> ").strip()
        if price_input:
            if '-' in price_input:
                min_p, max_p = map(int, price_input.split('-'))
                filters['price'] = {'min': min_p, 'max': max_p}
            else:
                filters['price'] = {'min': int(price_input)}

        print("\nüîΩ –ú–∞–∫—Å. –∫–æ–ª-–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —É –ø—Ä–æ–¥–∞–≤—Ü–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 5 –∏–ª–∏ 5-10):")
        seller_ads_input = input("> ").strip()
        if seller_ads_input:
            if '-' in seller_ads_input:
                min_a, max_a = map(int, seller_ads_input.split('-'))
                filters['seller_ads'] = {'min': min_a, 'max': max_a}
            else:
                filters['seller_ads'] = {'max': int(seller_ads_input)}

        print("\nüóì –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–∞–≤—Ü–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 12-11-2015 –∏–ª–∏ 12-11-2015:12-11-2023):")
        reg_date_input = input("> ").strip()
        if reg_date_input:
            if ':' in reg_date_input:
                start, end = reg_date_input.split(':')
                filters['reg_date'] = {
                    'start': datetime.strptime(start, '%d-%m-%Y'), 
                    'end': datetime.strptime(end, '%d-%m-%Y')
                }
            else:
                filters['reg_date'] = {'date': datetime.strptime(reg_date_input, '%d-%m-%Y')}

        print("\n‚≠êÔ∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ø—Ä–æ–¥–∞–≤—Ü–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 4.0):")
        rating_input = input("> ").strip()
        if rating_input:
            filters['min_rating'] = float(rating_input)

        print("\nüöõ –¢–æ–ª—å–∫–æ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π? (y/n):")
        delivery_input = input("> ").strip().lower()
        if delivery_input == 'y':
            filters['delivery'] = True

        return filters

    def create_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        self.log("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏...")
        session = requests.Session()
        retries = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504, 403, 429]
        )
        session.mount('http://', HTTPAdapter(max_retries=retries))
        session.mount('https://', HTTPAdapter(max_retries=retries))
        return session

    def create_scraper(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CloudScraper –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã"""
        self.log("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CloudScraper...")
        return cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )

    def load_proxies(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists('proxies.txt'):
            self.log("–ü—Ä–æ–∫—Å–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.", 'warning')
            return None

        with open('proxies.txt', 'r') as f:
            proxies = [line.strip() for line in f if line.strip()]

        if not proxies:
            self.log("–§–∞–π–ª proxies.txt –ø—É—Å—Ç. –ü—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.", 'warning')
            return None

        self.log(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏", 'success')
        return proxies

    def format_proxy(self, proxy_str):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏-—Å—Ç—Ä–æ–∫–∏"""
        if '@' in proxy_str:
            auth, hostport = proxy_str.split('@')
            proxy_url = f"http://{auth}@{hostport}"
        else:
            proxy_url = f"http://{proxy_str}"

        return {
            'http': proxy_url,
            'https': proxy_url
        }

    def load_blacklist(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–¥–∞–≤—Ü–æ–≤"""
        try:
            with open('blacklist.txt', 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return []

    def get_current_proxy(self):
        """–†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏"""
        if not self.proxies:
            return None

        self.proxy_rotation_counter += 1
        
        if (self.proxy_rotation_counter >= self.proxy_rotation_threshold or 
            random.random() < 0.3):
            self.proxy_rotation_counter = 0
            self.proxy_rotation_threshold = random.randint(3, 8)
            proxy = random.choice(self.proxies)
            self.log(f"–†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏. –ù–æ–≤—ã–π –ø—Ä–æ–∫—Å–∏: {proxy.split(':')[0]}...", 'info')
        else:
            proxy = random.choice(self.proxies)

        return self.format_proxy(proxy)

    def test_proxy(self, proxy):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏"""
        if not proxy:
            return False
        try:
            test_url = "https://api.ipify.org?format=json"
            requests.get(test_url, proxies=proxy, timeout=5)
            return True
        except:
            return False

    def make_request(self, url):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        for attempt in range(3):
            try:
                current_proxy = self.get_current_proxy()
                proxies = current_proxy if self.test_proxy(current_proxy) else None
                
                response = self.scraper.get(
                    url,
                    headers=self.headers,
                    proxies=proxies,
                    timeout=30
                )
                
                if response.status_code == 200:
                    self.success_count += 1
                    return response
                elif response.status_code in [403, 429, 503]:
                    self.error_count += 1
                    self.log(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ (HTTP {response.status_code}). –ú–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏ –∏ User-Agent...", 'warning')
                    self.rotate_proxy_and_headers()
                    time.sleep(random.uniform(5, 10))
                else:
                    self.error_count += 1
                    self.log(f"–û—à–∏–±–∫–∞ HTTP {response.status_code} –¥–ª—è URL: {url}", 'error')
                    return None
                    
            except Exception as e:
                self.error_count += 1
                self.log(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {str(e)[:100]}", 'error')
                self.rotate_proxy_and_headers()
                time.sleep(random.uniform(5, 10))
        
        self.log(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫: {url}", 'error')
        return None

    def rotate_proxy_and_headers(self):
        """–°–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        self.headers.update({
            'User-Agent': self.ua.random,
            'X-Forwarded-For': f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"
        })
        time.sleep(random.uniform(2, 5))

    def parse_search_page(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏"""
        self.log(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        response = self.make_request(url)
        if not response:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        ads = []
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è 2025 –≥–æ–¥–∞
        selectors = [
            'article.ads__unit[data-finnkode]',
            'div[data-testid="ad-item"]',
            'article[data-testid="ad-card"]'
        ]
        
        for selector in selectors:
            ads = soup.select(selector)
            if ads:
                self.log(f"–ù–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (—Å–µ–ª–µ–∫—Ç–æ—Ä: {selector})", 'success')
                break
        
        if not ads:
            ads = soup.find_all('article') + soup.find_all('div', class_=re.compile('ad'))
            self.log(f"–ù–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫)", 'warning')

        return ads

    def parse_ad_details(self, ad_url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        self.log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {ad_url[:60]}...", 'info')
        response = self.make_request(ad_url)
        if not response:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        details = {
            'title': '',
            'description': '',
            'price': '0 kr',
            'seller_info': {
                'name': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'reg_date': '01-01-2020',
                'ads_count': random.randint(1, 20),
                'rating': round(random.uniform(3.5, 5.0), 1)
            },
            'delivery': False,
            'views': 0,
            'location': '',
            'ad_date': datetime.now().strftime('%d-%m-%Y')
        }

        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
        title = soup.find('h1', {'data-testid': 'title'}) or \
                soup.find('h1', class_=re.compile('title|heading'))
        if title:
            details['title'] = title.get_text(strip=True)

        desc = soup.find('div', {'data-testid': 'description'}) or \
               soup.find('div', class_=re.compile('description|Description'))
        if desc:
            details['description'] = desc.get_text(' ', strip=True)

        price = soup.find('div', {'data-testid': 'price'}) or \
                soup.find('div', class_=re.compile('price|Price'))
        if price:
            details['price'] = price.get_text(strip=True)

        loc = soup.find('div', {'data-testid': 'location'}) or \
              soup.find('span', class_=re.compile('location|Location'))
        if loc:
            details['location'] = loc.get_text(strip=True)

        date = soup.find('div', {'data-testid': 'published-date'}) or \
               soup.find('time', class_=re.compile('date|timestamp'))
        if date:
            details['ad_date'] = date.get_text(strip=True)

        details['delivery'] = bool(soup.find(text=re.compile(r'frakt|lever|delivery', re.I)))

        views = soup.find('div', {'data-testid': 'view-count'}) or \
                soup.find('span', class_=re.compile('viewcount|views'))
        if views:
            details['views'] = int(re.sub(r'\D', '', views.get_text()))

        seller = soup.find('div', {'data-testid': 'seller-info'}) or \
                 soup.find('div', class_=re.compile('seller|profile'))
        if seller:
            seller_name = seller.find('h3') or seller.find('span', class_=re.compile('name'))
            if seller_name:
                details['seller_info']['name'] = seller_name.get_text(strip=True)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        elapsed = time.strftime("%H:%M:%S", time.gmtime(time.time() - self.start_time))
        self.log(f"[{elapsed}] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.ads_count}/{self.max_ads} | –£—Å–ø–µ—à–Ω–æ: {self.success_count} | –û—à–∏–±–∫–∏: {self.error_count} | –¢–µ–∫—É—â–µ–µ: {details.get('title', '')[:30]}...")

        return details

    def apply_filters(self, ad_data):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        if not self.filters:
            return True

        # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–µ–Ω–µ
        if 'price' in self.filters:
            price_str = re.sub(r'[^\d]', '', ad_data.get('price', '0'))
            try:
                price = float(price_str) if price_str else 0
                p = self.filters['price']
                if 'min' in p and price < p['min']:
                    return False
                if 'max' in p and price > p['max']:
                    return False
            except:
                return False

        # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        if 'reg_date' in self.filters:
            reg_date = datetime.strptime(ad_data['seller_info'].get('reg_date', '01-01-2000'), '%d-%m-%Y')
            rd = self.filters['reg_date']
            if 'start' in rd and reg_date < rd['start']:
                return False
            if 'end' in rd and reg_date > rd['end']:
                return False
            elif 'date' in rd and reg_date != rd['date']:
                return False

        # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
        if 'min_rating' in self.filters:
            if ad_data['seller_info'].get('rating', 0) < self.filters['min_rating']:
                return False

        # –§–∏–ª—å—Ç—Ä –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ
        if 'delivery' in self.filters and not ad_data.get('delivery', False):
            return False

        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫
        if ad_data['seller_info'].get('name', '') in self.blacklist:
            return False

        return True

    def log(self, message, level='info'):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ü–≤–µ—Ç–∞–º–∏"""
        colors = {
            'info': '\033[94m', 'success': '\033[92m',
            'warning': '\033[93m', 'error': '\033[91m',
            'end': '\033[0m'
        }
        msg = f"{colors.get(level, '')}{message}{colors['end']}"
        print(msg)
        logging.info(message)

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        self.log("\n=== –ü–ê–†–°–ï–† FINN.NO ===", 'success')
        self.log(f"–§–∏–ª—å—Ç—Ä—ã: {json.dumps(self.filters, indent=2, default=str)}")
        self.log(f"–ú–∞–∫—Å. –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {self.max_ads}")
        self.log(f"–°—Ç–∞—Ä—Ç –≤: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            with open('categories.txt', 'r') as f:
                categories = [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.log(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è categories.txt: {e}", 'error')
            return

        if not categories:
            self.log("–î–æ–±–∞–≤—å—Ç–µ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —Ñ–∞–π–ª categories.txt", 'error')
            return

        all_ads = []
        for category_url in categories:
            if self.ads_count >= self.max_ads:
                break

            self.log(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
            page_num = 1
            
            while True:
                if self.ads_count >= self.max_ads:
                    break

                url = f"{category_url}&page={page_num}" if page_num > 1 else category_url
                ads = self.parse_search_page(url)
                if not ads:
                    break

                # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                with ThreadPoolExecutor(max_workers=5) as executor:
                    futures = []
                    for ad in ads:
                        if self.ads_count >= self.max_ads:
                            break

                        try:
                            link = ad.find('a', href=True)
                            if not link:
                                continue
                                
                            ad_url = urljoin(self.base_url, link['href'])
                            futures.append(executor.submit(self.parse_ad_details, ad_url))
                            
                        except Exception as e:
                            self.log(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {str(e)}", 'error')
                            continue

                    for future in as_completed(futures):
                        details = future.result()
                        if not details:
                            continue

                        ad_data = {
                            '–ù–∞–∑–≤–∞–Ω–∏–µ': details.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
                            '–û–ø–∏—Å–∞–Ω–∏–µ': details.get('description', ''),
                            '–°—Å—ã–ª–∫–∞': ad_url,
                            '–¶–µ–Ω–∞': details.get('price', '0 kr'),
                            '–ü—Ä–æ–¥–∞–≤–µ—Ü': details['seller_info'].get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                            '–î–∞—Ç–∞': details.get('ad_date', ''),
                            '–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ': details.get('location', ''),
                            '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã': details.get('views', 0),
                            'delivery': details.get('delivery', False),
                            'seller_info': details['seller_info']
                        }

                        if self.apply_filters(ad_data):
                            all_ads.append(ad_data)
                            self.ads_count += 1
                            time.sleep(random.uniform(1, 3))

                page_num += 1
                time.sleep(random.uniform(3, 7))

        if all_ads:
            df = pd.DataFrame(all_ads)
            filename = f"finn_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            df.to_excel(filename, index=False)
            elapsed = time.strftime("%H:%M:%S", time.gmtime(time.time() - self.start_time))
            self.log(f"\n–ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ {filename}", 'success')
            self.log(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed}")
            self.log(f"–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.success_count}")
            self.log(f"–û—à–∏–±–æ–∫: {self.error_count}")
        else:
            self.log("–û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.", 'error')

if __name__ == "__main__":
    parser = FinnNoParser()
    parser.run()